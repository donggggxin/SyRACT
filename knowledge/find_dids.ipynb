{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8bb219-fa98-49be-b7a9-3324c6cb0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先查找同时含有mesh的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161bc38-3a4a-4ed0-a542-3e0ccad59879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_did(h,t):\n",
    "    #print(h)\n",
    "    #print(t)\n",
    "    url = f'https://pubmed.ncbi.nlm.nih.gov/?term=({h}[Title/Abstract])+AND+({t}[Title/Abstract])'\n",
    "    # 发送HTTP请求\n",
    "    time.sleep(1)\n",
    "    response = requests.get(url)\n",
    "    dids = []\n",
    "    # 检查请求是否成功\n",
    "    if response.status_code == 200:\n",
    "        # 使用BeautifulSoup解析HTML内容\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #print(soup)\n",
    "        pmid_tags = soup.find_all('span', class_='docsum-pmid')\n",
    "\n",
    "        # 遍历所有找到的标签并打印其内容\n",
    "        for tag in pmid_tags:\n",
    "            #print(tag.get_text().strip())\n",
    "            dids_i = tag.get_text().strip()  # 使用strip()移除首尾空白字符\n",
    "            dids.append(dids_i)\n",
    "    else:\n",
    "        print('Failed to retrieve the webpage')\n",
    "\n",
    "    #print(dids)\n",
    "    return dids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c258a-3558-4dc6-aae2-9cff86dd0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果没有，分别查找含有某一个mesh的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba48f4a-5dd8-48cb-9fd4-026f516bddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_did(a):\n",
    "    #print(h)\n",
    "    #print(t)\n",
    "    url = f'https://pubmed.ncbi.nlm.nih.gov/?term=({a}[Mesh]))'\n",
    "    # 发送HTTP请求\n",
    "    time.sleep(2)\n",
    "    response = requests.get(url)\n",
    "    dids = []\n",
    "    # 检查请求是否成功\n",
    "    if response.status_code == 200:\n",
    "        # 使用BeautifulSoup解析HTML内容\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #print(soup)\n",
    "        pmid_tags = soup.find_all('span', class_='docsum-pmid')\n",
    "\n",
    "        # 遍历所有找到的标签并打印其内容\n",
    "        for tag in pmid_tags:\n",
    "            #print(tag.get_text().strip())\n",
    "            dids_i = tag.get_text().strip()  # 使用strip()移除首尾空白字符\n",
    "            dids.append(dids_i)\n",
    "    else:\n",
    "        print('Failed to retrieve the webpage')\n",
    "\n",
    "    #print(dids)\n",
    "    return dids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75434db5-56a4-41c7-a14e-ea5a8c9d8a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeace1d-2fe2-4a83-b0de-9166ad2637b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(name,did):\n",
    "    print(name)\n",
    "    file=f'{name}.json'\n",
    "    did = json.dumps(did, indent=4)\n",
    "    print(file)\n",
    "# 将JSON字符串写入文件\n",
    "    with open(file, 'w') as json_file:\n",
    "        json_file.write(did)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
